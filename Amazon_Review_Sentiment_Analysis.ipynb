{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5c9a7aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in c:\\python310\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\python310\\lib\\site-packages (from gdown) (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\python310\\lib\\site-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: filelock in c:\\python310\\lib\\site-packages (from gdown) (3.8.2)\n",
      "Requirement already satisfied: tqdm in c:\\python310\\lib\\site-packages (from gdown) (4.64.1)\n",
      "Requirement already satisfied: six in c:\\python310\\lib\\site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\python310\\lib\\site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests[socks]->gdown) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\python310\\lib\\site-packages (from requests[socks]->gdown) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests[socks]->gdown) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests[socks]->gdown) (3.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\python310\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from tqdm->gdown) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Python310\\python.exe -m pip install --upgrade pip' command.\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-WZKE5xHw-3m_SL_PtOgwkzdFROIWqih\n",
      "To: C:\\Users\\KIIT\\Desktop\\ML\\reviews.csv\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 301M/301M [01:41<00:00, 2.98MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'reviews.csv'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install gdown\n",
    "import pandas as pd\n",
    "#import files from google drive\n",
    "import gdown\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1-WZKE5xHw-3m_SL_PtOgwkzdFROIWqih'\n",
    "output = 'reviews.csv'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6d7a012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(output)\n",
    "#we will only look at reviews where score is > 0\n",
    "raw_df = raw_df[raw_df['Score'] > 0]\n",
    "df = raw_df[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7710c22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = raw_df[:2000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7634b64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "from collections import defaultdict\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tag_map = defaultdict(lambda : wordnet.NOUN)\n",
    "tag_map['J'] = wordnet.ADJ\n",
    "tag_map['V'] = wordnet.VERB\n",
    "tag_map['R'] = wordnet.ADV\n",
    "\n",
    "def preprocess_with_lemmatization(document):\n",
    "  tokens = nltk.word_tokenize(document.lower())\n",
    "  tagged_tokens = nltk.pos_tag(tokens)\n",
    "  lemmas = []\n",
    "  for token, pos in tagged_tokens:\n",
    "    lemmatizer_tag = tag_map[pos[0]]\n",
    "    lemma = lemmatizer.lemmatize(token, pos=lemmatizer_tag)\n",
    "    lemmas.append(lemma)\n",
    "  return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eb0f2203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default dict starts counting from 0 when its a new word\n",
    "from collections import defaultdict\n",
    "\n",
    "#no of times a word appears in a five star review\n",
    "five_star_counts = defaultdict(int)\n",
    "one_star_counts = defaultdict(int)\n",
    "# total_counts = defaultdict(int)\n",
    "total_counts = defaultdict(lambda: 10)\n",
    "# lambda: 10 finds word in the document which occurs at least 10 times\n",
    "\n",
    "\n",
    "#zip helps in iterating through the two lists at the same time\n",
    "for text, score in zip(df['Text'], df['Score']):\n",
    "  tokens = preprocess_with_lemmatization(text)\n",
    "  for token in tokens:\n",
    "    if score == 5:\n",
    "      five_star_counts[token] += 1\n",
    "    elif score == 1:\n",
    "      one_star_counts[token] += 1\n",
    "    total_counts[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "57d3747b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'be', 'the', ',', 'i', 'and', 'a', 'to', 'it', 'of']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(five_star_counts.keys(),key=lambda token: five_star_counts[token], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "257491fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'be', 'the', 'i', ',', 'and', 'a', 'it', 'to', 'of']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(one_star_counts.keys(), key=lambda token: one_star_counts[token], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ca66a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "# Term\n",
    "# Frequency\n",
    "# Inverse\n",
    "# Document \n",
    "# Frequency\n",
    "# We count the number of times a word appears and divide it with the nummber of times it appears anywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aa1c0886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['highly',\n",
       " 'dog',\n",
       " 'great',\n",
       " 'love',\n",
       " 'delicious',\n",
       " 'wonderful',\n",
       " 'recommend',\n",
       " '--',\n",
       " 'weight',\n",
       " 'ever']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding out good tokens\n",
    "# We categorize unique words only available on tokens in five star reviews\n",
    "five_star_normalized = {}\n",
    "\n",
    "for token in five_star_counts:\n",
    "    normalized_five_star_count = five_star_counts[token] / total_counts[token]\n",
    "    five_star_normalized[token] = normalized_five_star_count\n",
    "    \n",
    "good_tokens = sorted(five_star_normalized.keys(),key=lambda token: five_star_normalized[token], reverse=True)[:10]\n",
    "good_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6772c49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['highly',\n",
       "  'dog',\n",
       "  'great',\n",
       "  'love',\n",
       "  'delicious',\n",
       "  'wonderful',\n",
       "  'recommend',\n",
       "  '--',\n",
       "  'weight',\n",
       "  'ever',\n",
       "  'thank',\n",
       "  'snack',\n",
       "  'fast',\n",
       "  'our',\n",
       "  'best',\n",
       "  'thanks',\n",
       "  'perfect',\n",
       "  'calorie',\n",
       "  'she',\n",
       "  '!'],\n",
       " ['alive',\n",
       "  'frosting',\n",
       "  'aware',\n",
       "  'return',\n",
       "  'awful',\n",
       "  'throw',\n",
       "  'horrible',\n",
       "  'donut',\n",
       "  'disappointed',\n",
       "  'cacao',\n",
       "  'garbage',\n",
       "  'maltitol',\n",
       "  'picture',\n",
       "  'stale',\n",
       "  'frost',\n",
       "  'change',\n",
       "  'label',\n",
       "  'terrible',\n",
       "  'cancel',\n",
       "  'clearly'])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_star_normalized = {}\n",
    "\n",
    "for token in five_star_counts:\n",
    "  normalized_five_star_count = five_star_counts[token] / total_counts[token]\n",
    "  five_star_normalized[token] = normalized_five_star_count\n",
    "\n",
    "one_star_normalized = {}\n",
    "\n",
    "for token in one_star_counts:\n",
    "  normalized_one_star_count = one_star_counts[token] / total_counts[token]\n",
    "  one_star_normalized[token] = normalized_one_star_count\n",
    "\n",
    "good_tokens = sorted(five_star_normalized.keys(), key=lambda token: five_star_normalized[token], reverse=True)[:20]\n",
    "bad_tokens = sorted(one_star_normalized.keys(), key=lambda token: one_star_normalized[token], reverse=True)[:20]\n",
    "\n",
    "good_tokens,bad_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1c7fb478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c2f67202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_135440\\1893550487.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Prediction'] = df['Text'].apply(predict_stars)\n"
     ]
    }
   ],
   "source": [
    "def predict_stars(document):\n",
    "    # We'll start with \"neutral\"\n",
    "    sentiment = 0\n",
    "    # Our default preprocessing will be just splitting along spaces, but\n",
    "    # we can pass in a custom preprocess script later if we want\n",
    "    tokens = preprocess_with_lemmatization(document)\n",
    "\n",
    "    # We loop through all the tokens in our document\n",
    "    for token in tokens:\n",
    "        # If the token is one of our \"good\" ones, we'll add 1 to the sentiment\n",
    "        if token in good_tokens:\n",
    "            sentiment += 1\n",
    "        # If the token is one of our \"bad\" ones, we'll subtract 1 from the sentiment\n",
    "        if token in bad_tokens:\n",
    "            sentiment -= 1\n",
    "    \n",
    "    if sentiment > 1:\n",
    "      return 5\n",
    "    elif sentiment == 1:\n",
    "      return 4\n",
    "    elif sentiment == 0:\n",
    "      return 3\n",
    "    elif sentiment == -1:\n",
    "      return 2\n",
    "    else:\n",
    "      return 1\n",
    "\n",
    "df['Prediction'] = df['Text'].apply(predict_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3e36fe65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "   Prediction  \n",
       "0           5  \n",
       "1           2  \n",
       "2           5  \n",
       "3           3  \n",
       "4           5  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6c347e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5285"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_cases = sum(df['Prediction'] == df['Score'])\n",
    "total_cases = len(df['Prediction'])\n",
    "accuracy = correct_cases / total_cases \n",
    "accuracy\n",
    "\n",
    "# 22.00  -->  33.34  -->  52.85 for first 1000\n",
    "# if you increase the number of test cases to 2000 the accuracy should probably increase\n",
    "# human prediction should be about 20.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081804f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358aa345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "62e7f7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'dog', 'like', 'love', 'you']\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "          \"You like dogs\",\n",
    "          \"You like cats\",\n",
    "          \"You love dogs\",\n",
    "          \"You love cats\",\n",
    "]\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "for doc in corpus:\n",
    "  tokens = preprocess_with_lemmatization(doc)\n",
    "  vocab.update(tokens)\n",
    "    \n",
    "#this will update all of our individual tokens in the set\n",
    "#we then turned our vocab to a list and then sorted it\n",
    "vocab = list(vocab)\n",
    "vocab.sort()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "05ce29a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D: number of docs in our corpus\n",
    "# T: maximum number of tokens in any document (T = 100)\n",
    "\n",
    "word_to_index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "  word_to_index[word] = i\n",
    "\n",
    "#enumerate helps in going through the list of vocabs and gives the list of indices\n",
    "#we associate indices to every word present in the vocab\n",
    "\n",
    "vectors = []\n",
    "for doc in corpus: # D\n",
    "  tokens = preprocess_with_lemmatization(doc)\n",
    "  vector = [0] * len(vocab)\n",
    "  for token in tokens: # T\n",
    "    index = word_to_index[token]\n",
    "    vector[index] += 1\n",
    "  vectors.append(vector)\n",
    "\n",
    "# O(D)\n",
    "#maximum number of tokens in a word remains constant for which time complexity remains as O(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "94bea74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': 0, 'dog': 1, 'like': 2, 'love': 3, 'you': 4}\n",
      "\n",
      "You like dogs\n",
      "You like cats\n",
      "You love dogs\n",
      "You love cats\n",
      "\n",
      "[0, 1, 1, 0, 1]\n",
      "[1, 0, 1, 0, 1]\n",
      "[0, 1, 0, 1, 1]\n",
      "[1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#unpacks the vector and prints with a seprator \\n\n",
    "print(word_to_index)\n",
    "print()\n",
    "print(*corpus,sep=\"\\n\")\n",
    "print()\n",
    "print(*vectors, sep=\"\\n\")\n",
    "#after we converted our word_to_index we got five unique words so we vectorize it\n",
    "#In the first document we get 'you','like','dogs' which are labelled as 4 2 1 so we put 1 on those places rest as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4f6fb9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1]\n",
      " [1 0 1 0 1]\n",
      " [0 1 0 1 1]\n",
      " [1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#count vectorization \n",
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(corpus)\n",
    "#what fir_transform does is what we did manually fit actually finds unique words and appends it to the vocab\n",
    "#trnasform helps in transforming the words into vector notation\n",
    "print(vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0401359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df[:10000]\n",
    "training_df = raw_df[:9000]\n",
    "testing_df = raw_df[9000:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5d907f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "training_vectors = vectorizer.fit_transform(training_df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c509af6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "18141\n"
     ]
    }
   ],
   "source": [
    "first_vector = training_vectors.toarray()[0]\n",
    "print(first_vector)\n",
    "print(first_vector.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "65824e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "18141\n"
     ]
    }
   ],
   "source": [
    "testing_vectors = vectorizer.transform(training_df['Text'])\n",
    "first_testing_vector = testing_vectors.toarray()[0]\n",
    "print(first_testing_vector)\n",
    "print(first_testing_vector.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "66c1b4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000    5\n",
       "9001    1\n",
       "9002    2\n",
       "9003    5\n",
       "9004    5\n",
       "9005    4\n",
       "9006    4\n",
       "9007    3\n",
       "9008    5\n",
       "9009    3\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels = training_df['Score']\n",
    "testing_labels = testing_df['Score']\n",
    "\n",
    "training_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7d0f79d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(training_vectors,training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "89c343e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9462222222222222"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_classifier.score(training_vectors,training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe92c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c00b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
